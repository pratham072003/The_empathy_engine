# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s4bvLOPFfvIdL3xHJqto349cMJA92Co7
"""

!pip install fastapi uvicorn pydantic

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from fastapi.responses import FileResponse
import os
import uuid
from transformers import pipeline
from pydub import AudioSegment, effects
from openai import OpenAI
from google.colab import userdata

# Initialize FastAPI
app = FastAPI(title="The Empathy Engine API")

# Setup directories
OUTPUT_DIR = "outputs"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Initialize models
emotion_classifier = pipeline(
    "text-classification",
    model="cardiffnlp/twitter-roberta-base-sentiment-latest",
    return_all_scores=True
)

client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))

# Voice profiles for emotions
VOICE_PROFILES = {
    "happy": {"rate_multiplier": 1.25, "pitch_semitones": +4, "volume_db": +3},
    "neutral": {"rate_multiplier": 1.0, "pitch_semitones": 0, "volume_db": 0},
    "frustrated": {"rate_multiplier": 0.85, "pitch_semitones": -3, "volume_db": -2},
}

# Request model
class TextInput(BaseModel):
    text: str


# ==== Helper Functions ====
def detect_emotion(text: str):
    result = emotion_classifier(text)[0]
    best = max(result, key=lambda x: x['score'])
    label = best['label'].lower()
    confidence = best['score']
    if label == "positive":
        return "happy", confidence
    elif label == "negative":
        return "frustrated", confidence
    else:
        return "neutral", confidence


def change_pitch(audio: AudioSegment, semitones: float) -> AudioSegment:
    """
    Changes pitch of an AudioSegment by given semitones.
    Positive semitones = higher pitch, negative = lower pitch.
    """
    if semitones == 0:
        return audio  # no pitch change needed

    # Calculate new sample rate
    new_sample_rate = int(audio.frame_rate * (2.0 ** (semitones / 12.0)))

    # Apply new sample rate, then reset to original frame rate
    shifted = audio._spawn(audio.raw_data, overrides={"frame_rate": new_sample_rate})
    return shifted.set_frame_rate(audio.frame_rate)


def change_speed(audio: AudioSegment, speed: float) -> AudioSegment:
    """
    Changes playback speed (rate) of an AudioSegment.
    speed > 1.0 = faster, speed < 1.0 = slower.
    """
    if speed == 1.0:
        return audio  # no speed change needed

    # Change frame rate to change speed
    new_frame_rate = int(audio.frame_rate * speed)
    faster = audio._spawn(audio.raw_data, overrides={"frame_rate": new_frame_rate})
    return faster.set_frame_rate(audio.frame_rate)


def apply_volume(audio: AudioSegment, db_change: float) -> AudioSegment:
    """
    Adjusts volume of an AudioSegment by given dB change.
    Positive dB = louder, negative dB = softer.
    """
    if db_change == 0:
        return audio
    return audio + db_change


def synthesize_with_emotion(text: str):
    # 1. Detect emotion
    emotion, confidence = detect_emotion(text)
    profile = VOICE_PROFILES.get(emotion, VOICE_PROFILES['neutral'])

    # 2. Create output path
    out_filename = f"emp_{uuid.uuid4().hex[:10]}.mp3"
    out_path = os.path.join(OUTPUT_DIR, out_filename)

    # 3. Stream audio response directly to file
    with client.audio.speech.with_streaming_response.create(
        model="gpt-4o-mini-tts",
        voice="verse",
        input=text,
    ) as response:
        response.stream_to_file(out_path)

    # 4. Apply pitch/speed/volume changes
    audio = AudioSegment.from_file(out_path, format="mp3")
    audio = change_pitch(audio, profile['pitch_semitones'])
    audio = change_speed(audio, profile['rate_multiplier'])
    audio = apply_volume(audio, profile['volume_db'])
    audio.export(out_path, format="mp3")

    return out_path, emotion, confidence


# ==== FastAPI Routes ====
@app.post("/api/empathy")
def empathy_endpoint(input: TextInput):
    text = input.text
    try:
        audio_path, emotion, confidence = synthesize_with_emotion(text)
        return {
            "emotion": emotion,
            "confidence": round(confidence, 3),
            "audio_url": f"/api/audio/{os.path.basename(audio_path)}"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/audio/{filename}")
def get_audio(filename: str):
    file_path = os.path.join(OUTPUT_DIR, filename)
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail="File not found")
    return FileResponse(file_path, media_type="audio/mpeg")

#!pip install pyngrok

import nest_asyncio
from pyngrok import ngrok
import uvicorn
import __main__

# Apply nest_asyncio for Colab
nest_asyncio.apply()

# Start ngrok tunnel
public_url = ngrok.connect(8000)
print("Public API URL:", public_url)

# Start FastAPI server
uvicorn.run(__main__.app, host="0.0.0.0", port=8000)

